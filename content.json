{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"轰炸大西瓜","url":"http://yoursite.com","root":"/"},"pages":[{"title":"tags","date":"2019-08-10T14:47:40.000Z","updated":"2019-08-10T14:48:51.369Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"强化学习复习01","slug":"强化学习复习01","date":"2019-08-21T14:43:13.000Z","updated":"2019-08-22T11:30:20.026Z","comments":true,"path":"2019/08/21/强化学习复习01/","link":"","permalink":"http://yoursite.com/2019/08/21/强化学习复习01/","excerpt":"","text":"1.举出强化学习与有监督学习的异同点。有监督学习靠样本标签训练模型，强化学习靠的是什么？在监督学习和无监督学习中，数据都是静止的、不需要和环境进行互动，只要给定了一定量的样本，那么就能够对其进行学习。而强化学习中，agent需要和环境不断互动来获取数据，数据是动态的。 2.强化学习解决的是什么样的问题？序列决策问题 3.多臂老虎机只有一个序列，为什么也是MDP过程多臂老虎机其实可以看作一种特殊的强化学习问题：序列长度一直为1的强化学习。它在强化学习关键的三要素（状态，动作，奖惩）中，没有状态一项，也可以说，它当前的动作不会改变环境状态，亦或者说它的最优动作是全局的，只要找到了这个动作，那一直采取这个动作就可以保证最大的累计回报。 4.强化学习的损失函数是什么max \\int{p(\\tau)R(\\tau)}=min\\quad -\\int p(\\tau)R(\\tau)也就是最大化累计折扣奖励 5.POMDP是什么？ 考虑动作 不考虑动作 状态完全可见 MDP MC 状态不完全可见 POMDP HMM 由此可见就是状态不完全可见的马尔可夫决策过程。 部分观测马尔可夫决策过程(POMDP)就是MDP和HMM结合的产物。在POMDP中，agent不能够直接观测到当前的状态。但是在每一个时刻中，agent能够得到状态的观测值。agent根据这些观测值去思考当前的状态到底是什么（the agent uses these observations to form a belief of in what state the system currently is.）而猜测的状态是一种概率分布(this belief is called a belief state and is expressed as a probability distribution over the states).POMDP的目标就是去找到基于这些猜测状态的最优策略（the solution of the POMDP is a policy prescribing which action is optimal for each belief state）。 一个POMDP可以由一个7元组描述： $S$:是一个有限状态的集合 $A$:是有限动作的集合 $P$:是状态转移矩阵,每一个元素是$p_{ss’}^{a}$:在状态$s$时执行动作$a$之后转移到$s’$的概率 $R:S \\times A \\to \\mathbb{R}$是奖励函数 $\\Omega$:是观测值的集合 $O$:是执行了动作$a$,进入到下一个状态$s’$得到的观察值$o$的概率$O(o|s’, a)$ $\\lambda$: 折扣因子 注意上面的O，是执行力动作、进入了下一个状态下观测到某个观测的概率. 资料：吴恩达对于增强学习的形象论述（下） 6.马尔科夫决策过程是什么？里面的“马尔科夫”体现了什么性质？马尔科夫过程是什么？马尔可夫决策过程就是可以被描述成一个5元组$(S,A,R,P,\\gamma)$，分别是有限的状态集合、有限的动作集合、奖励函数、状态转移矩阵、折扣因子（和上面的差不多）。马尔可夫性质即齐次马尔科夫性： p(s_{t+1}|s_{t},s_{t-1},s_{t-2}.....) = p(s_{t+1}|s_{t})也就是说下一个状态发生的概率只是和当前的状态相关，当前的状态包含了所有的历史信息。马尔可夫过程可以被描述成一个2元组$(S,P)$,也就是有限的状态集合、状态转移矩阵。 7.贝尔曼方程 8.最优值函数和最优策略为什么等价？强化学习的目标可以大致上理解为：找到一个能够在长期运行的前提下获得尽可能最多奖励的的策略，对于有限长的的MDP来说，最优策略可以如下定义： 值函数定义了策略的偏序关系，一个策略$\\pi$的好坏不低于另外的一个策略$\\pi’$的定义是其期望累计奖励大于等于另外一个策略的期望累计奖励。 \\pi \\geqslant \\pi'只有当 v_{\\pi} \\geqslant v_{\\pi'} \\quad for\\ all\\ s\\ in\\ S的时候才成立 。也就是说，至少存在一个策略优于或者是等于剩下的策略。这就是最佳策略optimal policy。虽然有很多个这样的策略，但是都可以表示成$\\pi$。他们均共享同样的状态值函数，也就是最优值函数*optimal state value function,表示为$v{\\pi}$总的来说，最优策略是依赖于最优值函数的。最优策略可以从最优值函数获取。 9.如果不满足马尔科夫性怎么办？当前时刻的状态和它之前很多很多个状态都有关。引入RNN，没看 10.求解马尔科夫决策过程都有哪些方法？有模型用什么方法？动态规划是怎么回事？解决MDP问题主要有3个方法 动态规划 Dynamic Programming 基于值函数的方法 Value Based Method 基于策略的方法 Policy Based Method 当有模型的时候使用动态规划的方法（model-based）。 10.1强化学习中的动态规划当已经知道完全的MDP，也就是5元组$(S,A,R,P,\\gamma)$。而当5元组全部知道后，可以采用策略迭代Policy Iteration和值迭代Value Iteration来求解MDP过程。利用动态规划求解问题的前提是： 整个优化问题可以被分解为多个子问题 子问题的最优解可以被存储和重复使用","categories":[],"tags":[{"name":"强化学习","slug":"强化学习","permalink":"http://yoursite.com/tags/强化学习/"}]},{"title":"一些问题","slug":"一些问题","date":"2019-08-21T06:26:59.000Z","updated":"2019-08-22T12:18:02.404Z","comments":true,"path":"2019/08/21/一些问题/","link":"","permalink":"http://yoursite.com/2019/08/21/一些问题/","excerpt":"","text":"1. 线性回归的若干描述 问题：关于线性回归的描述,以下正确的有:(B C E)A. 基本假设包括随机干扰项是均值为0,方差为1的标准正态分布B. 基本假设包括随机干扰项是均值为0的同方差正态分布C. 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量D. 在违背基本假设时,模型不再可以估计E. 可以用DW检验残差是否存在序列相关性F. 多重共线性会使得参数估计值方差减小 一元线性回归的基本假设： 随机误差项是一个期望值或平均值为0的随机变量 对于解释变量的所有观测值，随机误差项有相同的方差 随机误差项服从正态分布 随机误差项彼此不相关 解释变量是确定性变量，不是随机变量，与随机误差项之间彼此独立 解释变量之间不存在精确的（完全的）线性关系，即解释变量的样本观测值矩阵是满秩矩阵 随机干扰项是均值为0的同方差正态分布 当存在异方差（违背了基本假设）时，OLS（普通最小二乘法）有如下的问题：参数估计值时无偏的，但是不是最小方差线性无偏估计 在违背基本假设时,普通最小二乘法估计量不再是最佳线性无偏估计量 DW检验，一种统计分析中常用的的检验序列一阶自相关最常用的方法 所谓多重共线性是指线性回归模型中的解释白能量之间存在精确的相关关系或高度相关关系从而使模型估计失真。 多重共线性使参数估计值的方差增大 2. 参数估计方法 MLE MAP Bayesian Estimation 概率是已知模型和参数，推数据。统计时已知数据，推模型和参数 贝叶斯公式：$p(\\theta|X) = \\cfrac{p(X|\\theta)*p(\\theta)}{p(X)}$ 这个公式也叫做逆概率公式，可以将后验概率转换为基于似然函数和先验概率的计算表达形式：$posterior = \\cfrac{likelihood*prior}{evidence}$ MLE 极大似然估计MLE使用似然函数取到最大值时候的参数值作为估计值，似然函数可以写成：$L(\\theta|X) = p(X|\\theta) = \\prod p(X=x|\\theta) $ 最大似然估计等价于最小化训练集上的经验分布和模型分布之间的差异，即最小化这两个分布的交叉熵。 MAP 最大后验概率最大后验概率和最大似然估计相似，不同的地方在于估计$\\theta$的函数加入先验$p(\\theta)$,也就是说，这个时候要求的不是似然函数最大，而是要求整个后验概率最大（evidence不会发生变化，所以后验和似然乘以先验的结果成正比）。 MAP假定参数$\\theta$服从某一种分布，并不是固定的值，但最后估计出来的参数仍然是一个确定的值 $\\hat{\\theta}{MAP} = argmax{\\theta} \\cfrac{p(X|\\theta)p(\\theta)}{p(X)} = argmax_{\\theta}p(X|\\theta)p(\\theta)$ 贝叶斯估计贝叶斯时最大后验概率的扩展，和MAP一样，贝叶斯估计也不认为参数是固定的，而是服从某一个先验分布，但是MAP是直接估计出$\\theta$的值，而贝叶斯估计是估计出参数的分布，这就是贝叶斯和MLE以及MAP的最大不同。 Batch Normalization神经网络训练过程的本质是学习数据分布，如果训练数据和测试数据的分布不同将大大降低网络的泛化能力，因此我们需要在训练开始之前对所有的输入数据进行归一化处理。然而随着网络训练的进行，每一个隐层的参数变化使得后一层的输入也发生变化，从而使每一批训练数据的分布也随着改变，致使网络在每一次迭代的过程中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。BN是针对每一批次的数据，在网络的每一层输入之前增加归一化处理(均值为0，方差为1)，将所有批数据强制在统一的数据分布下即对该层的任意一个神经元$\\hat{x}^{(k)}$采用 \\hat{x}^{(k)} = \\cfrac{ x^{(k)} - E[x^{(k)}] }{\\sqrt{ Var[x^{(k)}] }}其中$x^{(k)}$为该层第k个神经元的原始输入,$E[x^{(k)}]$为该层这一批次数据的均值，$\\sqrt{ Var[x^{(k)}]}$为这一批数据在第k个神经元的标准差。BN可以看作是每一层输入和上一层的输出中新加入了一个新的计算层，对数据的分布进行额外的约束，从而增强模型的泛化能力。但是同时，BN也降低了模型的拟合能力，归一化之后的输入分布被强制为0均值和1标准差。以Sigmoid为例，BN之后数据整体处于函数的非饱和区域，只包含线性变换，破坏了以前学习到的特征分布。 卷积和池化之后的图像大小计算(input\\_size + 2*padding - kernel\\_size)/stride + 1 = output\\_size卷积向下取整、池化向上取整 stride为1kernel为3 padding为1kernel为5 padding为3卷积前后大小不变","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"集成学习","slug":"集成学习","date":"2019-08-21T05:31:41.000Z","updated":"2019-08-21T06:17:07.534Z","comments":true,"path":"2019/08/21/集成学习/","link":"","permalink":"http://yoursite.com/2019/08/21/集成学习/","excerpt":"","text":"1. 集成学习中的方差和偏差集成学习利用多个基分类器组合形成一个总分类器，能够达到单个基分类器达不到的效果。 平均方法： 例如随机森林、Bagging。在平均方法中，系统同时训练多个基分类器，分类器之间没有联系。在分类或者是回归时，结果根据各个分类器的结果去综合出最后的结果，比如投票。 提升方法： 例如GDBT、Adaboost，每个分类器由前一个得到，通过组合的形式得到最终的分类器。 平均方法： 尝试去降低方差提升方法： 尝试去降低偏差 如何去理解： boosting算法的基本上就是在去优化loss function，而优化loss function的过程就是降低bias，这使得variance升高。bagging算法之所以进行bagging，就是希望模型具有更好的鲁棒性，显然这就是减少variance，使得bias提升 2. 为什么随机森林的数的深度往往大于GDBT树的深度？随机森林属于bagging算法，而GDBT属于boosting算法。由问题1可以得知： bagging算法的bias高，variance低。 boosting算法的bias低， variance高。 所以对于随机森林（以及更一般的bagging算法）来说，要使用更深的树，以便增强每个基学习器的学习能力（bias降低）。同理对于GDBT（以及更一般的boosting算法）来说，要使用更浅的树，来保证variance的降低（模型复杂度降低，variance降低）。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"网络","slug":"网络","date":"2019-08-20T08:53:26.000Z","updated":"2019-08-21T14:44:23.356Z","comments":true,"path":"2019/08/20/网络/","link":"","permalink":"http://yoursite.com/2019/08/20/网络/","excerpt":"","text":"OSI与TCP/IP各层的结构与功能,都有哪些协议五层协议的体系结构———-假装这里有图 OSI的体系结构 应用层 表示层 会话层 运输层 网络层 数据链路层 物理层 TCP/IP的体系结构 应用层 : FTP、SMTP、TELNET 运输层 :TCP、UDP 网际层（网络层） : IP 网络接口层 五层协议的体系结构 应用层 运输层 网络层 数据链路层 物理层 网络体系结构和协议1 应用层应用层的任务是通过应用进程之间的交互来完成特定的网络应用，应用层协议定义的是应用进程间的通信和交互规则，不同的网络应用应该使用不同的应用层协议，比如：DNS、HTTP、SMTP应用层之间交互的数据单元：报文 2 运输层运输层的主要任务就是负责向 两台主机进程之间 的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。运输层的主要协议： 运输控制协议TCP：提供面向连接的可靠的数据传输服务 用户数据协议UDP：提供无连接的，尽最大努力的数据传输服务（不保证数据传输可靠性） TCP的特点(x5)： TCP是面向连接的 每一条TCP连接只能有两个端点，每一条TCP连接只能是点对点的 TCP提供可靠交付的服务。通过TCP连接传送的数据，无差错、不丢失、不重复、按序到达 TCP提供全双工通信，TCP允许通信双方在任何时候都能够发送数据。TCP连接的两端都设有发送缓存和接收缓存，用来存放双方的通信。 面向字节流。TCP中的流指的是流入进程或者从进程流出的字节序列。“面向字节流”的意思：虽然应用程序和TCP的交互是一个一个的数据块（大小不等），但是TCP把应用程序浇下来的数据看作一连串无结构的字节流 所谓的端点就是套接字socket: (ip:port) UDP的特点(x6) UDP是无连接的 UDP使用尽可能最大努力交付，（不保证可靠交付），因此不需要维持复杂的连接状态 UDP是面向报文的 UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低 UDP支持一对一、一对多、多对多、多对一的交互通信 UDP的首部开销小，只有8个字节，TCP有20个字节 3 网络层网络层负责为分组交换网上的不同主机提供通信服务。在发送数据的时候，网络层把运输层产生的报文段或用户数据报封装成分组或包进行传送。在TCP/IP协议中，由于网络层使用IP协议，因此分组也叫ip数据报，简称数据报。网络层的另外一个任务就是选择合适的路由，使源主机运输层所传下来的分株，能够通过网络层中的路由器找到目的主机。互联网是一个很大的网络，它通过大量的异构网络通过路由器进行相互连接，因特网主要的网络层协议是无连接的网际协议IP和其他的路由选择协议，所以网络层又叫做IP层或者是网际层。 4 数据链路层数据链路层简称链路层，两台主机之间的传输总是在一段一段的链路上传送的，这就需要使用专门的链路层协议。数据链路层将网络层所交下来的IP数据报组装成帧，在两个响铃的节点之间的链路上进行传送帧，每一帧都包含有数据和控制信息。控制信息的作用：1：控制信息能够让接收端知道一帧从哪一个比特开始到哪一个比特结束。从而获取数据。2：控制信息可以检测收到的帧是否有错误，同时可以进行纠错。 5 物理层在物理层上所传输的数据单位是比特，物理层的作用是实现相邻计算机节点之间比特流的透明传输，尽可能屏蔽掉具体传输介质和物理设备的差异。 TCP三次握手和四次挥手TCP的三次握手 client发送 SYN=1, seq=x server发送 SYN=1， ACK=1, seq=y, ack=x+1 client发送 ACK=1，seq=x+1, ack=y+1 问题1：为什么还要发送一个请求?答：为了防止已经失效的连接请求报文突然又传送到了B，B会误以为A又请求一次连接，并向A确认，而A不会对其进行回复，因此导致B的网络资源被浪费。 问题2：为什么要传回SYN？答：接收端传回发送端所发送的SYN是为了钙塑发送端，我接收的信息确实就是你所发送的信息。 问题3： 传了SYN为什么还要传ACK？答:双方必须保证两者互相发送信息都无误，传了SYN，说明发送方到接收方的通道没有问题，而接收方到发送方还需要ACK信号进行验证。 TCP的四次挥手 A发送 FIN=1, seq=u u是A前面已经传输过的数据的最后一个字节序号加上1 B发送 ACK=1, seq=v, ack=u+1 v是B前面已经传输过的最后一个字节序号加上1 B发送 FIN=1, ACK=1, seq=w, ack=u+1 w是由于在半关闭的情况下，B继续发送了数据，同时确认号必须重复上次发送的序号 A发送 ACK=1, seq=u+1, ack=w+1 TCP和UDP的区别 类型 是否面向连接 是否可靠传输 传输形式 传输效率 所需资源 应用场景 首部字节 TCP 是 是 字节流 低 多 要求通信可靠（比如邮件） 20~60 UDP 否 否 数据报文段 高 少 要求速度快（域名转换、视频直播） 8 TCP协议如何保证可靠传输 应用程序被分割成TCP认为最适合发送的数据块。 TCP会对发送的包进行编号，接收方会对接收到的包进行排序，以保证接受得到的数据为有序的。 校验和： TCP将保持它首部和数据的校验和，这是一个端到端的校验和，目的是检测数据在传输过程种的变化。如果收到的校验和有差错，TCP将会丢弃和不确认接受这个给报文段。 TCP会丢弃重复的数据 流量控制： TCP的发送发和接收方都设置有固定大小的缓冲区域，TCP的接收端只允许发送端发送接收缓冲区能够容纳的数据，当接收方来不及处理接收到的数据时，会通知发送方降低发送速率，防止包丢失。（TCP使用滑动窗口实现流量控制） 拥塞控制: 当网络拥塞的时候减少数据的发送。 停止等待协议: 为了实现可靠传输，基本原理就是每发完一个分组就停止发送，等待接收方确认消息，在收到确认之后再发下一个分组。超时重传： 当TCP发送一个包之后，启动一个定时器，如果不能及时收到回复的话，重新发送。 停止等待协议 停止等待协议是为了实现可靠传输的，它的基本原理就是发完一个分组就等待接收方发回确认消息，在确认收到的情况下，再继续发送下一个分组。 1. 无差错情况 1. A发送分组M1 2. B确认收到分组M1,向A发送确认信息 3. A接收到B发送的确认信息，继续发送分组M2 4. 依次类推 2. 出现差错 1. A发送分组M1 2. B接收到分组M1,发现错误，什么也不做，直接丢弃分组M1 3. A这个时候超过一定的时间没有收到确认，认为刚才发送的分组丢失，重传刚才发送的分组（**超时重传**） 注意事项： A在发送一个分组之后必须要暂时保留已经发送的分组的副本 分组和确认分组必须进行编号 超时计时器设置的重传时间必须设置的比数据在分组传输中的平均往返时间更长一点 3.确认丢失 1. A发送了分组M1 2. B接收到了分组M1，并发回M1的确认 3. A超时重传M1 4. B收到了重传的M1，**丢弃当前的分组**，不向上级交付。同时**向A发送确认** 4.确认迟到 1. A发送了分组M1 2. B收到了分组M1，并发回M1的确认 3. B的确认在网络中阻塞，没有及时地送回A 4. A超时重传 5. B接收到了重新发送的分组M1和确认丢失一样，立即丢弃当前分组，同时回传确认 6. A收到了B发挥的确认，继续发送下一个分组 7. A收到了前一次迟到的确认，并且**立即丢弃** 上述的可靠传输协议成为自动重传请求ARP 5.连续ARQ协议由于ARQ的信道利用率太低，因此使用连续ARQ协议，实现是滑动窗口。发送方维持一个发送窗口，位于发送窗口中的所有分组都可以连续发送出去，不必要等待确认的传回。这样信道利用率就上去了。连续ARQ协议规定，发送方每收到一个确认就把发送窗口向前滑动一个分组的位置，接受党一般采取累计确认的方式：接受方不必对收到的分组逐个发送确认，而是接收几个分组之后，对按序到达的最后一个分组发送请求，说明到这个分组为止的所有分组已经正确接收 网络状态码 类别 原因短语 1XX Informational 信息性状态码 接受的请求正在处理 2XX success 成功状态码 请求正常处理完毕 3XX redirection 重定向状态码 需要进行附加操作以完成请求 4XX Client Error 客户端错误状态码 服务器无法处理请求 5XX Server Error 服务器错误状态码 服务器处理请求出错 常用端口号 应用程序 FTP* TELNET* SMTP* DNS TFTP HTTP* SNMP SNMP(trap) 端口号 21 23 25 53 69 80 161 162 服务端使用的端口号系统端口号： 0~1024登记端口号：1024~49151 客户端使用的端口号客户端使用的端口号： 49152~65535","categories":[],"tags":[{"name":"计算机网络","slug":"计算机网络","permalink":"http://yoursite.com/tags/计算机网络/"}]},{"title":"复习1","slug":"复习1","date":"2019-08-20T06:10:05.000Z","updated":"2019-08-20T08:44:02.807Z","comments":true,"path":"2019/08/20/复习1/","link":"","permalink":"http://yoursite.com/2019/08/20/复习1/","excerpt":"","text":"python语言特性1 可变类型和不可变类型 可变类型 不可变类型 list string dict tuple set numbers 2 参数默认值的变化1234567def f(l=[]): print(l) l.append(2) l.append(5) f() #[]f() #[2,5] &lt;-----在这个地方可以发现默认参数l已经指向了[2,5] 3 @staticmethod &amp; @classmethodpython中有三个方法： 实体方法 静态方法(staticmethod) 类方法(classmethod) 1234567891011class A: def entity_method(self): #实体方法，传self参数 ...... @staticmethod def static_method(): #静态方法 ...... @classmethod def class_method(cls): #类方法,传cls参数 ...... 三种方法的使用： / 实例方法 类方法 静态方法 a = A() a.entity_method() a.class_method() a.static_method() A 不能用 A.class_method() A.static_method() 4 类变量和实例变量12345class A: num_of_instance = 0 #类变量 def __init__(self, name): self.name = name #这个就是实例变量 A.num_of_instance += 1 #注意是 A.类变量名 的用法 5 单下划线和双下划线__func__:一种约定，python内部的名字，和用户自己起的名字进行区分(__init__, __del__,__call__)_func:一种约定，用来指定变量私有，不能用from module import *来进行导入 __func: 解析器用 _classname__func来对这个名字进行替换，以便于区分其他类（与子类）相同的命名6 迭代器和生成器问题：将列表生成式中的[]改成()之后数据结构是否发生改变?ans: 是，从列表编程生成器 12345a = [x for x in range(10)]print(a) #[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]b = (x for x in range(10))print(b) #&lt;generator object &lt;genexpr&gt; at 0x000001B1FBEAC678&gt; 为什么：通过列表生成式，可以直接创建一个列表，但是受到内存的影响，列表容量有限。同时，创建一个包含百万元素的列表时，我们可能只需要访问前面的几个元素，后面的元素所占有的空间都是浪费的。所以在python中：可以采用生成器 生成器生成器是一种特殊的程序，可以被用作控制循环的迭代行为，生成器是迭代器的一种。生成器和迭代器","categories":[],"tags":[]},{"title":"快速幂","slug":"快速幂","date":"2019-08-11T03:43:52.000Z","updated":"2019-08-21T14:43:54.213Z","comments":true,"path":"2019/08/11/快速幂/","link":"","permalink":"http://yoursite.com/2019/08/11/快速幂/","excerpt":"","text":"快速幂比如2的11次方可以算作 2^(1011) = 2^(8)2^(2)2^(1)用一个临时变量存储当前的结果，每当遇到指数当前位为1的情况就和当前的2^(k)相乘，最后返回结果k用另外一个临时变量保存，每次和他自己相乘 123456789101112131415161718192021222324class Solution: def Power(self, base, exponent): if exponent == 0: return 1 exp = exponent #辅助变量 result = 1 temp = base if exp &lt; 0: if base == 0: raise RuntimeError(&apos;valueError&apos;) exp = - exp while exp != 0: if exp &amp; 1 == 1: result *= temp temp *= temp exp = exp &gt;&gt; 1 if exponent &lt; 0: return 1/result else: return result","categories":[],"tags":[{"name":"编程题","slug":"编程题","permalink":"http://yoursite.com/tags/编程题/"}]}]}